{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LevenshteinDistance(s1,s2):\n",
    "    \"\"\"calcul the distance of Levenshtein bewteen s1 and s2\"\"\"\n",
    "    \n",
    "    D = np.zeros(( len(s1)+1 , len(s2)+1 ), dtype = int)\n",
    "    \n",
    "    for i in range( len(s1)+1 ):\n",
    "        D[i][0] = i\n",
    "    \n",
    "    for j in range( len(s2)+1 ):\n",
    "        D[0][j] = j\n",
    "    \n",
    "    #print(D)\n",
    "    \n",
    "    for i in range(1, len(s1)+1):\n",
    "        for j in range(1, len(s2)+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            D[i][j] = min( D[i-1][j]+1, D[i][j-1]+1, D[i-1][j-1]+cost )\n",
    "            \n",
    "    #print(D)\n",
    "    \n",
    "    return D[len(s1)][len(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentComparison(s1,s2,lim):\n",
    "    \"\"\"\n",
    "    Require : String s1 and s2\n",
    "    Require : A threshold value lim\n",
    "    Ensure : Are s1 and s2 compatible ?\n",
    "    \"\"\"\n",
    "    \n",
    "    #delete '.'\n",
    "    s1 = s1.replace('.',' ')\n",
    "    s1 = s1.replace('-',' ')\n",
    "    s2 = s2.replace('.',' ')\n",
    "    s2 = s2.replace('-',' ')\n",
    "    \n",
    "    #split by ' '\n",
    "    c1 = s1.split()\n",
    "    c2 = s2.split()\n",
    "    \n",
    "    n1 = len(c1)\n",
    "    n2 = len(c2)\n",
    "    \n",
    "    #compare the first fragment\n",
    "    if ( len(c1[0])>1  and len(c2[0])>1 ):\n",
    "        if ( LevenshteinDistance(c1[0],c2[0]) > lim ):\n",
    "            #print(1)\n",
    "            return False\n",
    "    else:\n",
    "        if ( len(c1[0])>1 ):\n",
    "            if (c1[0][0] != c2[0]):\n",
    "                #print(2)\n",
    "                return False\n",
    "        else:\n",
    "            if (c1[0] != c2[0][0]):\n",
    "                #print(3)\n",
    "                return False\n",
    "    \n",
    "    #compare the last fragment\n",
    "    if ( len( c1[n1-1] )>1 and len( c2[n2-1] )>1 ):\n",
    "        if ( LevenshteinDistance(c1[n1-1], c2[n2-1]) > lim):\n",
    "            #print(4)\n",
    "            return False\n",
    "    else:\n",
    "        #print(5)\n",
    "        return False\n",
    "    \n",
    "    #compare the rest fragments\n",
    "    Mark_c1 = [False] * n1\n",
    "    Mark_c2 = [False] * n2\n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( len( c1[i] )>1 and len( c2[j] )>1 and LevenshteinDistance(c1[i], c2[j])<lim ):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c1[i])) and len( c1[i] )>1 and len(c2[j]) == 1 and c1[i][0] == c2[j]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c2[j])) and len( c2[j] )>1 and len(c1[i]) == 1 and c1[i] == c2[j][0]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c1[i])) and (not(Mark_c2[j])) and len(c1[i]) == 1 and len( c2[j] ) == 1  and c1[i] == c2[j]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    #check whether at least one string has all fragments marked\n",
    "    for i in range(1, n1-1):\n",
    "        if ( (not(Mark_c1[i])) ):\n",
    "            for j in range(1, n2-1):\n",
    "                if( (not(Mark_c2[j])) ):\n",
    "                    #print(6)\n",
    "                    return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_stream in d:\\softwares\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: requests in d:\\softwares\\anaconda3\\lib\\site-packages (from json_stream) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install json_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_stream\n",
    "\n",
    "def sortShortAndLongNameRecords(g):\n",
    "    \"\"\"\n",
    "    Require: Ambiguous group g (the file json g opened)\n",
    "    Ensure: List S of clusters of authorship records (write a file json S)\n",
    "    \"\"\"\n",
    "    outputShort_name = \".\\shortNameRecordsOf\" + g.name.strip(\".\\\\\")\n",
    "    outputShort = open(outputShort_name,'w+',encoding=\"utf-8\")\n",
    "    outputLong_name = \".\\longNameRecordsOf\" + g.name.strip(\".\\\\\")\n",
    "    outputLong = open(outputLong_name,'w+',encoding=\"utf-8\")\n",
    "    \n",
    "    outputShort.write('[')\n",
    "    outputLong.write('[')\n",
    "    \n",
    "    authorshipRecords = json_stream.load(g)\n",
    "    \n",
    "    author = \"\"\n",
    "    lastName = \"\"\n",
    "    coauthors = []\n",
    "    defaultTitle = \"\"\n",
    "    venue = \"\"\n",
    "    duplicId = 0\n",
    "    Dict = {}\n",
    "    \n",
    "    nb_longName = 0\n",
    "    nb_shortName = 0\n",
    "\n",
    "    for authorshipRecord in authorshipRecords :\n",
    "    \n",
    "        author = authorshipRecord[\"author\"]\n",
    "        \n",
    "        #test the name is in short format or not\n",
    "        L_author = author.split(' ')\n",
    "        output = outputLong\n",
    "        nb_longName += 1\n",
    "        for i in range(len(L_author)):\n",
    "            #if the i-th fragement of the name is shorter than 2\n",
    "            if (len(L_author[i]) <= 2):\n",
    "                output = outputShort\n",
    "                \n",
    "                if (nb_shortName != 0):\n",
    "                    outputShort.write(',')\n",
    "                    \n",
    "                nb_shortName += 1\n",
    "                nb_longName -= 1\n",
    "                break\n",
    "                \n",
    "        if (output == outputLong and nb_longName != 0):\n",
    "            outputLong.write(',')\n",
    "        \n",
    "        lastName = authorshipRecord[\"last name\"]\n",
    "        coauthors = [ coauthor for coauthor in authorshipRecord[\"coauthors\"] ]\n",
    "        defaultTitle = authorshipRecord[\"defaultTitle\"]\n",
    "        venue = authorshipRecord[\"venue\"]\n",
    "        duplicId = authorshipRecord[\"duplicId\"]\n",
    "        \n",
    "        Dict = {}\n",
    "        Dict[\"author\"] = author\n",
    "        Dict[\"last name\"] = lastName\n",
    "        Dict[\"coauthors\"] = coauthors\n",
    "        Dict[\"defaultTitle\"] = defaultTitle\n",
    "        Dict[\"venue\"] = venue\n",
    "        Dict[\"duplicId\"] = duplicId\n",
    "        #print(Dict)\n",
    "        json.dump(Dict, output, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    outputShort.write(']')\n",
    "    outputLong.write(']')\n",
    "    \n",
    "    outputShort.close()\n",
    "    outputLong.close()\n",
    "    \n",
    "    print(\"Il y a\", nb_longName, \"Authorship Record de long nom\")\n",
    "    print(\"Il y a\", nb_shortName, \"Authorship Record de court nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = open(r\".\\ambiguousGroup.json\",'r',encoding=\"ISO-8859-1\")\n",
    "\n",
    "# sortShortAndLongNameRecords(g)\n",
    "\n",
    "# g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "def mkdir(path): \n",
    "    \"\"\"\n",
    "    path exemple: .\\\\firstStep\n",
    "    \"\"\"\n",
    "    folder = os.path.exists(path)\n",
    " \n",
    "    if not folder:                   \n",
    "        os.makedirs(path)            \n",
    "        return True\n",
    " \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_stream\n",
    "import os\n",
    "\n",
    "def processList (A, Ci, folderAddress):\n",
    "    \"\"\"\n",
    "    Require: list A of authorship records (the file json A opened)\n",
    "    Require: list Ci of authorship record clusters named by the first author name\n",
    "    Require: the addresse of the file of the Ambiguous Group\n",
    "    Ensure: list C0 of authorship record clusters named by the first author name\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lim = 2 #threshold value lim of LevenshteinDistance\n",
    "    \n",
    "    L_severalARs = [] #list of the numbers of the files which have several Autorchip Records\n",
    "    \n",
    "    mkdir(folderAddress)\n",
    "    \n",
    "    authorshipRecords = json_stream.load(A)\n",
    "    \n",
    "    C0 = Ci\n",
    "    for authorshipRecord in authorshipRecords:\n",
    "        \n",
    "        author = authorshipRecord[\"author\"]\n",
    "        lastName = authorshipRecord[\"last name\"]\n",
    "        coauthors = [ coauthor for coauthor in authorshipRecord[\"coauthors\"] ]\n",
    "        defaultTitle = authorshipRecord[\"defaultTitle\"]\n",
    "        venue = authorshipRecord[\"venue\"]\n",
    "        duplicId = authorshipRecord[\"duplicId\"]\n",
    "        \n",
    "        Dict = {}\n",
    "        Dict[\"author\"] = author\n",
    "        Dict[\"last name\"] = lastName\n",
    "        Dict[\"coauthors\"] = coauthors\n",
    "        Dict[\"defaultTitle\"] = defaultTitle\n",
    "        Dict[\"venue\"] = venue\n",
    "        Dict[\"duplicId\"] = duplicId\n",
    "        \n",
    "        inserted = False\n",
    "        i = 0\n",
    "        while (not inserted and i < len(C0)):\n",
    "            c = C0[i]\n",
    "            #if the author name from a is similar with author name from the first authorship record of c\n",
    "            if (fragmentComparison(author,c,lim)):\n",
    "                #if it exists a coauthor name in a that is similar with some coauthor name in c\n",
    "                file_check = open(folderAddress + \"\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'r', encoding=\"utf-8\")\n",
    "                authorshipRecords_check = json_stream.load(file_check)\n",
    "                coauthors_check = [] #the list of the coauthor names in c\n",
    "                \n",
    "                for authorshipRecord_check in authorshipRecords_check:\n",
    "                    for coauthor_check in authorshipRecord_check[\"coauthors\"]:\n",
    "                        coauthors_check.append(coauthor_check)\n",
    "                        \n",
    "                file_check.close()\n",
    "                \n",
    "                for j in range(len(coauthors)): #the j-th coauthor of authorship record a\n",
    "                    coauthorCompared = coauthors[j]\n",
    "                    for l in range(len(coauthors_check)): #the l-th coauthor \n",
    "                        if (fragmentComparison(coauthorCompared,coauthors_check[l],lim)):\n",
    "                            \n",
    "                            #delete ']'\n",
    "                            file_write = open(folderAddress + \"\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'rb+')\n",
    "                            file_write.seek(-1, os.SEEK_END)\n",
    "                            file_write.truncate()\n",
    "                            file_write.close()\n",
    "\n",
    "                            file_write = open(folderAddress + \"\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'a', encoding=\"utf-8\")\n",
    "                            file_write.write(',')\n",
    "                            json.dump(Dict, file_write, ensure_ascii=False, indent=4)\n",
    "                            file_write.write(']')\n",
    "                            file_write.close()\n",
    "\n",
    "                            L_severalARs.append(i)\n",
    "                            \n",
    "                            inserted = True\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                \n",
    "            i += 1\n",
    "        #a new cluster is created with this authorship record a\n",
    "        if (not inserted):\n",
    "            file_write = open(folderAddress + \"\\\\\" + str(len(C0)) + \"authorshipRecordCluster.json\", 'w', encoding=\"utf-8\")\n",
    "            file_write.write('[')\n",
    "            json.dump(Dict, file_write, ensure_ascii=False, indent=4)\n",
    "            file_write.write(']')\n",
    "            file_write.close()\n",
    "            \n",
    "            C0.append(author)\n",
    "            \n",
    "    L_severalARs = list(set(L_severalARs))\n",
    "    print(\"The numbers of the files which have several Autorchip Records\",L_severalARs)\n",
    "    \n",
    "    return C0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = open(r\".\\longNameRecordsOfambiguousGroup.json\",'r',encoding=\"ISO-8859-1\")\n",
    "# C1 = []\n",
    "# C2 = processList(L,C1)\n",
    "# L.close()\n",
    "\n",
    "# S = open(r\".\\shortNameRecordsOfambiguousGroup.json\",'r',encoding=\"ISO-8859-1\")\n",
    "# C3 = processList(S,C2)\n",
    "# print(C3)\n",
    "# S.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstStep(ambiguousGroupAddress):\n",
    "    \"\"\"\n",
    "    Require: Ambiguous group g (the addresse of the file of the Ambiguous Group)\n",
    "    Ensure: List C of clusters of authorship records\n",
    "    \"\"\"\n",
    "    \n",
    "    g = open(ambiguousGroupAddress,'r',encoding=\"utf-8\")\n",
    "    \n",
    "    ambiguousGroupName = g.name.strip(\".\\\\\")\n",
    "    ambiguousGroupName = ambiguousGroupName.rstrip(\".json\")\n",
    "    \n",
    "    sortShortAndLongNameRecords(g)\n",
    "    \n",
    "    g.close()\n",
    "    \n",
    "    folderAddress = \".\\\\firstStepOf\" + ambiguousGroupName\n",
    "    \n",
    "    outputLong_name = \".\\longNameRecordsOf\" + ambiguousGroupName + \".json\"\n",
    "    \n",
    "    L = open(outputLong_name,'r',encoding=\"utf-8\")\n",
    "    C1 = []\n",
    "    C2 = processList(L,C1,folderAddress)\n",
    "    L.close()\n",
    "\n",
    "    outputShort_name = \".\\shortNameRecordsOf\" + ambiguousGroupName + \".json\"\n",
    "    S = open(outputShort_name,'r',encoding=\"utf-8\")\n",
    "    C3 = processList(S,C2,folderAddress)\n",
    "    print(C3)\n",
    "    S.close()\n",
    "\n",
    "    return C3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 60 Authorship Record de long nom\n",
      "Il y a 209 Authorship Record de court nom\n",
      "The numbers of the files which have several Autorchip Records [0, 4, 5, 39, 8, 12, 17, 22, 24, 26, 30]\n",
      "The numbers of the files which have several Autorchip Records [0, 1, 35, 5, 9, 42, 43, 44, 49, 50, 17, 21, 53]\n",
      "['alain enjalbert', 'andreas enge', 'adeline enderle', 'andreas engel', 'agnès oude engberink', 'alexandru ene', 'alexandru eniu', 'andreas enge', 'agathe ensinas', 'axel enders', 'anders enemar', 'alin enache', 'adriana both engel', 'akio enders', 'ali akbar enayati', 'abdelatif ennaji', 'alireza entezari', 'aomar ennaciri', 'akihito endo', 'akira endo', 'anne endrizzi', 'abdelhadi ennajih', 'agnes engberink', 'anna-mart engelbrecht', 'alejandro enriquez-cabrera', 'alain enjalbert', 'alain enard', 'andrea endimiani', 'andreas enge', 'antje engelhardt', 'antje engelhardt', 'agnes enyedi', 'andreas enge', 'amanda eng', 'adela enache-angoulvant', 'alexander van engelen', 'andreas engert', 'anja engel', 'adela enache-angoulvant', 'antoine enfissi', 'anne marie endougou effa', 'adriano ensinas', 'a. enokizono', 'a. en naciri', 'a. enzenhöfer', 'aschwin h engelen', 'a. engert', 'a engler', 'a en naciri', 'a. enßlin', 'andrea c. encalada', 'a. ennigrou', 'aotmane en naciri', 'aschwin h. engelen', 'a. encinas', 'a. engelhardt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alain enjalbert',\n",
       " 'andreas enge',\n",
       " 'adeline enderle',\n",
       " 'andreas engel',\n",
       " 'agnès oude engberink',\n",
       " 'alexandru ene',\n",
       " 'alexandru eniu',\n",
       " 'andreas enge',\n",
       " 'agathe ensinas',\n",
       " 'axel enders',\n",
       " 'anders enemar',\n",
       " 'alin enache',\n",
       " 'adriana both engel',\n",
       " 'akio enders',\n",
       " 'ali akbar enayati',\n",
       " 'abdelatif ennaji',\n",
       " 'alireza entezari',\n",
       " 'aomar ennaciri',\n",
       " 'akihito endo',\n",
       " 'akira endo',\n",
       " 'anne endrizzi',\n",
       " 'abdelhadi ennajih',\n",
       " 'agnes engberink',\n",
       " 'anna-mart engelbrecht',\n",
       " 'alejandro enriquez-cabrera',\n",
       " 'alain enjalbert',\n",
       " 'alain enard',\n",
       " 'andrea endimiani',\n",
       " 'andreas enge',\n",
       " 'antje engelhardt',\n",
       " 'antje engelhardt',\n",
       " 'agnes enyedi',\n",
       " 'andreas enge',\n",
       " 'amanda eng',\n",
       " 'adela enache-angoulvant',\n",
       " 'alexander van engelen',\n",
       " 'andreas engert',\n",
       " 'anja engel',\n",
       " 'adela enache-angoulvant',\n",
       " 'antoine enfissi',\n",
       " 'anne marie endougou effa',\n",
       " 'adriano ensinas',\n",
       " 'a. enokizono',\n",
       " 'a. en naciri',\n",
       " 'a. enzenhöfer',\n",
       " 'aschwin h engelen',\n",
       " 'a. engert',\n",
       " 'a engler',\n",
       " 'a en naciri',\n",
       " 'a. enßlin',\n",
       " 'andrea c. encalada',\n",
       " 'a. ennigrou',\n",
       " 'aotmane en naciri',\n",
       " 'aschwin h. engelen',\n",
       " 'a. encinas',\n",
       " 'a. engelhardt']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstStep(\".\\\\ambiguousGroup.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'F', 'Arguin']\n"
     ]
    }
   ],
   "source": [
    "s = \"J-F. Arguin\"\n",
    "s = s.replace('-', ' ')\n",
    "s = s.replace('.', ' ')\n",
    "#s = s.replace('  ',' ')\n",
    "c = s.split()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
