{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LevenshteinDistance(s1,s2):\n",
    "    \"\"\"calcul the distance of Levenshtein bewteen s1 and s2\"\"\"\n",
    "    \n",
    "    D = np.zeros(( len(s1)+1 , len(s2)+1 ), dtype = int)\n",
    "    \n",
    "    for i in range( len(s1)+1 ):\n",
    "        D[i][0] = i\n",
    "    \n",
    "    for j in range( len(s2)+1 ):\n",
    "        D[0][j] = j\n",
    "    \n",
    "    #print(D)\n",
    "    \n",
    "    for i in range(1, len(s1)+1):\n",
    "        for j in range(1, len(s2)+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            D[i][j] = min( D[i-1][j]+1, D[i][j-1]+1, D[i-1][j-1]+cost )\n",
    "            \n",
    "    #print(D)\n",
    "    \n",
    "    return D[len(s1)][len(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragmentComparison(s1,s2,lim):\n",
    "    \"\"\"\n",
    "    Require : String s1 and s2\n",
    "    Require : A threshold value lim\n",
    "    Ensure : Are s1 and s2 compatible ?\n",
    "    \"\"\"\n",
    "    \n",
    "    #split by ' '\n",
    "    c1 = s1.split(' ')\n",
    "    c2 = s2.split(' ')\n",
    "    \n",
    "    n1 = len(c1)\n",
    "    n2 = len(c2)\n",
    "    \n",
    "    #compare the first fragment\n",
    "    if ( len(c1[0])>1  and len(c2[0])>1 ):\n",
    "        if ( LevenshteinDistance(c1[0],c2[0]) > lim ):\n",
    "            #print(1)\n",
    "            return False\n",
    "    else:\n",
    "        if ( len(c1[0])>1 ):\n",
    "            if (c1[0][0] != c2[0]):\n",
    "                #print(2)\n",
    "                return False\n",
    "        else:\n",
    "            if (c1[0] != c2[0][0]):\n",
    "                #print(3)\n",
    "                return False\n",
    "    \n",
    "    #compare the last fragment\n",
    "    if ( len( c1[n1-1] )>1 and len( c2[n2-1] )>1 ):\n",
    "        if ( LevenshteinDistance(c1[n1-1], c2[n2-1]) > lim):\n",
    "            #print(4)\n",
    "            return False\n",
    "    else:\n",
    "        #print(5)\n",
    "        return False\n",
    "    \n",
    "    #compare the rest fragments\n",
    "    Mark_c1 = [False] * n1\n",
    "    Mark_c2 = [False] * n2\n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( len( c1[i] )>1 and len( c2[j] )>1 and LevenshteinDistance(c1[i], c2[j])<lim ):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c1[i])) and len( c1[i] )>1 and len(c2[j]) == 1 and c1[i][0] == c2[j]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c2[j])) and len( c2[j] )>1 and len(c1[i]) == 1 and c1[i] == c2[j][0]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    for i in range(1, n1-1):\n",
    "        for j in range(1, n2-1):\n",
    "            if ( (not(Mark_c1[i])) and (not(Mark_c2[j])) and len(c1[i]) == 1 and len( c2[j] ) == 1  and c1[i] == c2[j]):\n",
    "                Mark_c1[i] = True\n",
    "                Mark_c2[j] = True\n",
    "    \n",
    "    #check whether at least one string has all fragments marked\n",
    "    for i in range(1, n1-1):\n",
    "        if ( (not(Mark_c1[i])) ):\n",
    "            for j in range(1, n2-1):\n",
    "                if( (not(Mark_c2[j])) ):\n",
    "                    #print(6)\n",
    "                    return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_stream in d:\\softwares\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: requests in d:\\softwares\\anaconda3\\lib\\site-packages (from json_stream) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\softwares\\anaconda3\\lib\\site-packages (from requests->json_stream) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install json_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_stream\n",
    "\n",
    "def sortShortAndLongNameRecords(g):\n",
    "    \"\"\"\n",
    "    Require: Ambiguous group g (the file json g opened)\n",
    "    Ensure: List S of clusters of authorship records (write a file json S)\n",
    "    \"\"\"\n",
    "    outputShort_name = \".\\shortNameRecordsOf\" + g.name.strip(\".\\\\\")\n",
    "    outputShort = open(outputShort_name,'w+',encoding=\"ISO-8859-1\")\n",
    "    outputLong_name = \".\\longNameRecordsOf\" + g.name.strip(\".\\\\\")\n",
    "    outputLong = open(outputLong_name,'w+',encoding=\"ISO-8859-1\")\n",
    "    \n",
    "    outputShort.write('[')\n",
    "    outputLong.write('[')\n",
    "    \n",
    "    authorshipRecords = json_stream.load(g)\n",
    "    \n",
    "    author = \"\"\n",
    "    coauthors = []\n",
    "    publicationYear = \"\"\n",
    "    enTitle = \"\"\n",
    "    frTitle = \"\"\n",
    "    Dict = {}\n",
    "    \n",
    "    nb_longName = 0\n",
    "    nb_shortName = 0\n",
    "\n",
    "    for authorshipRecord in authorshipRecords :\n",
    "    \n",
    "        author = authorshipRecord[\"author\"]\n",
    "        \n",
    "        #test the name is in short format or not\n",
    "        L_author = author.split(' ')\n",
    "        output = outputLong\n",
    "        nb_longName += 1\n",
    "        for i in range(len(L_author)):\n",
    "            #if the i-th fragement of the name is shorter than 2\n",
    "            if (len(L_author[i]) <= 2):\n",
    "                output = outputShort\n",
    "                \n",
    "                if (nb_shortName != 0):\n",
    "                    outputShort.write(',')\n",
    "                    \n",
    "                nb_shortName += 1\n",
    "                nb_longName -= 1\n",
    "                break\n",
    "                \n",
    "        if (output == outputLong and nb_longName != 0):\n",
    "            outputLong.write(',')\n",
    "        coauthors = [ coauthor for coauthor in authorshipRecord[\"coauthors\"] ]\n",
    "        publicationYear = authorshipRecord[\"publicationYear\"]\n",
    "        enTitle = authorshipRecord[\"enTitle\"]\n",
    "        frTitle = authorshipRecord[\"frTitle\"]\n",
    "        \n",
    "        Dict = {}\n",
    "        Dict[\"author\"] = author\n",
    "        Dict[\"coauthors\"] = coauthors\n",
    "        Dict[\"publicationYear\"] = publicationYear\n",
    "        Dict[\"enTitle\"] = enTitle\n",
    "        Dict[\"frTitle\"] = frTitle\n",
    "        #print(Dict)\n",
    "        json.dump(Dict, output, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    outputShort.write(']')\n",
    "    outputLong.write(']')\n",
    "    \n",
    "    outputShort.close()\n",
    "    outputLong.close()\n",
    "    \n",
    "    print(\"Il y a\", nb_longName, \"Authorship Record de long nom\")\n",
    "    print(\"Il y a\", nb_shortName, \"Authorship Record de court nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 295 Authorship Record de long nom\n",
      "Il y a 51 Authorship Record de court nom\n"
     ]
    }
   ],
   "source": [
    "g = open(r\".\\authorshipRecords_20000.json\",'r',encoding=\"ISO-8859-1\")\n",
    "\n",
    "sortShortAndLongNameRecords(g)\n",
    "\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_stream\n",
    "import os\n",
    "\n",
    "def processList (A, Ci):\n",
    "    \"\"\"\n",
    "    Require: list A of authorship records (the file json A opened)\n",
    "    Require: list Ci of authorship record clusters named by the first author name\n",
    "    Ensure: list C0 of authorship record clusters named by the first author name\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lim = 2 #threshold value lim of LevenshteinDistance\n",
    "    \n",
    "    authorshipRecords = json_stream.load(A)\n",
    "    \n",
    "    C0 = Ci\n",
    "    for authorshipRecord in authorshipRecords:\n",
    "        \n",
    "        author = authorshipRecord[\"author\"]\n",
    "        coauthors = [ coauthor for coauthor in authorshipRecord[\"coauthors\"] ]\n",
    "        publicationYear = authorshipRecord[\"publicationYear\"]\n",
    "        enTitle = authorshipRecord[\"enTitle\"]\n",
    "        frTitle = authorshipRecord[\"frTitle\"]\n",
    "        \n",
    "        Dict = {}\n",
    "        Dict[\"author\"] = author\n",
    "        Dict[\"coauthors\"] = coauthors\n",
    "        Dict[\"publicationYear\"] = publicationYear\n",
    "        Dict[\"enTitle\"] = enTitle\n",
    "        Dict[\"frTitle\"] = frTitle\n",
    "        \n",
    "        inserted = False\n",
    "        i = 0\n",
    "        while (not inserted and i < len(C0)):\n",
    "            c = C0[i]\n",
    "            #if the author name from a is similar with author name from the first authorship record of c\n",
    "            if (fragmentComparison(author,c,lim)):\n",
    "                #if it exists a coauthor name in a that is similar with some coauthor name in c\n",
    "                file_check = open(\".\\\\firstStep\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'r', encoding=\"ISO-8859-1\")\n",
    "                authorshipRecords_check = json_stream.load(file_check)\n",
    "                coauthors_check = [] #the list of the coauthor names in c\n",
    "                \n",
    "                for authorshipRecord_check in authorshipRecords_check:\n",
    "                    for coauthor_check in authorshipRecord_check[\"coauthors\"]:\n",
    "                        coauthors_check.append(coauthor_check)\n",
    "                        \n",
    "                file_check.close()\n",
    "                \n",
    "                for j in range(len(coauthors)): #the j-th coauthor of authorship record a\n",
    "                    coauthorCompared = coauthors[j]\n",
    "                    for l in range(len(coauthors_check)): #the l-th coauthor \n",
    "                        if (fragmentComparison(coauthorCompared,coauthors_check[l],lim)):\n",
    "                            \n",
    "                            #delete ']'\n",
    "                            file_write = open(\".\\\\firstStep\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'rb+')\n",
    "                            file_write.seek(-1, os.SEEK_END)\n",
    "                            file_write.truncate()\n",
    "                            file_write.close()\n",
    "\n",
    "                            file_write = open(\".\\\\firstStep\\\\\" + str(i) + \"authorshipRecordCluster.json\", 'a', encoding=\"ISO-8859-1\")\n",
    "                            file_write.write(',')\n",
    "                            json.dump(Dict, file_write, ensure_ascii=False, indent=4)\n",
    "                            file_write.write(']')\n",
    "                            file_write.close()\n",
    "\n",
    "                            inserted = True\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                \n",
    "            i += 1\n",
    "        #a new cluster is created with this authorship record a\n",
    "        if (not inserted):\n",
    "            file_write = open(\".\\\\firstStep\\\\\" + str(len(C0)) + \"authorshipRecordCluster.json\", 'w', encoding=\"ISO-8859-1\")\n",
    "            file_write.write('[')\n",
    "            json.dump(Dict, file_write, ensure_ascii=False, indent=4)\n",
    "            file_write.write(']')\n",
    "            file_write.close()\n",
    "            \n",
    "            C0.append(author)\n",
    "    return C0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dcns', 'Keruel Bernard', 'Pierre Nicolas', 'Obriot Bruno', 'Badra Mohamad', 'Borghol Badra Rouba', 'Hajjeh Ibrahim', 'Gardan Remy', 'Panaget', 'Touze Jean Marie', 'Thuaire Micheline', 'Thuaire Raymond', \"Al'Stom Transport Teknolodzhis\", 'Dagdag Selim', 'Boccard', 'Grgoire Philippe', \"D'Hayer Benoît\", 'Tibi Annick', 'Husson Marie-Caroline', 'Boudy Vincent', 'Dcns', 'Niot Stephane', 'Vouillat Jerome', 'Salles Bernard', 'Jullien Denis', 'Universite Paul Sabatier Toulouse Iii', 'Mirey Gladys', 'Vignard Julien', 'Benoist Jean-Claude', 'Chapel Julien', 'Moignard Jeremy', 'Boucher Mathieu', 'Jastrzebski Alain', 'Bricaud Herve', 'Injection Haute Precision', 'Djian Francis', 'Proteor', 'Bellon Bernard', 'Thales', 'Queinnec Jean-Yves', 'Valeo Systemes Thermiques', 'Aircelle', 'Bardy Julien', 'Duchamp Boris', 'Veyet Frederick', 'Msika Philippe', 'Legran Zhak', 'Laboratoires Expanscience', 'Garnier Sebastien', 'Expanscience Lab', 'Msika Filipp', 'Legrand Jacques', 'Auger Aurelien', 'Alcatel Lucent', 'Boets Patrick', 'Dupuis Nicolas', 'Sanofi', 'Csatáriné Nagy Marianna', 'Friesz Antal', 'Sanofi Aventis', 'Shefer Khans-Lyudvig', 'Boshajnen Oliver', 'Dreyer Matthias', 'Habermann Paul', 'Boscheinen Oliver', 'Rao Ercole', 'Sommerfeld Mark', 'Schaefer Hans-Ludwig', 'Interdigital Madison Patent Holdings', 'Thomson Licensing', 'Zhang Tao', 'Barratt Christopher', 'Ciais Pascal', 'Insight Sip Sas', 'Mutu Zhan-Ljuk', 'Lafay Jean', 'Hybrigenics', \"Pellegrino Zhil'\", 'Moutou Jean-Luc', 'Muton Floran', 'Pellegrino Gilles', 'Chaussee Thomas', 'Deheunynck Damien', 'Backer Michael', 'Smits Valerie', 'Dow Corning Corporation', 'Onodera Satoshi', 'Schmittheisler Christophe', 'Gries Jean-Philippe', 'Mahle Behr France Rouffach S.A.S.', 'Rodriguez Luis Felipe', 'Nagares, S.A.', 'Khjubshle Tomas', 'Kaderajt Diter', 'Hachtel Stephanie', \"Ditrikh Aksel'\", 'Hiss Katrin', 'Dietrich Axel', 'Huebschle Thomas', 'Schaefer Matthias', \"Khakhtel' Shtefani\", 'Melet Pierre-Etienne', 'Patoureaux Marc', 'Dourthe Cedric', 'Dufresne Thierry', 'Guo Yang', 'Liu Yong', 'Thomson Licensing', 'Ancora Andrea', 'Brunel Dominique', 'Oct Circuit Technologies International Limited', 'Noel Laurent', 'Gssc Inc', 'Tur Charles', 'Reigneau Mathieu', 'Stmicroelectronics (R&D) Ltd.', 'Campbell Colin', 'Stmicroelectronics Research Development Limited', 'Gille Andreas', 'Sfar Safouane', 'St-Ericsson (France) Sas', 'Amor-Gueret Mounira', 'Institut Curie', 'Pruvot Thomas', 'Zimmermann Cecile', 'Sieber Michael', 'Smoby Toys S.A.S', 'Dupire Pierre', 'Husson Geoffroy', 'Valet Nicolas', 'Gauthier Sebastien', 'Brown Stuart Cameron', 'Van Wallenburg Andrea Christine', 'Vwb S.A.R.L', 'Videmann Antoine', 'Roptehn Vensan', 'Gelly Gerard', 'Videmann Antuan', 'Essilor International', 'Essilor Int', 'Buchon Cédric', 'Bjushon Sedrik', 'Zhelli Zherar', 'Roptin Vincent', 'Scheiper Bodo', 'Shajper Bodo', 'Wirth Klaus', 'Shrojder Kherman', 'Schreuder Herman', 'Pernerstorfer Josef', 'Horstick Georg', 'Sadowski Thorsten', 'Buning Christian', 'Olpp Thomas', 'Ruf Sven', 'Andersen Gert', 'Agusti Géraldine', 'Chevalier Yves', 'Doelker Eric', 'Nyffenegger Coralie', 'Jordan Olivier', 'Ogden David', 'Acher Francine', 'Dalko Peter', 'Petit Morgane', 'Reuzeau Christophe', 'Russinova Jenny', 'Universiteit Gent', 'Vib Vzw', 'Basf Plant Science Company Gmbh', 'Cephalon France', 'Nguyen Thanh-Tam', 'Sanofi-Aventis', 'Roach Arthur', 'Merck Serono S.A.', 'Weiner David', 'Bartoszyk Gerd', 'Von Raison Florian', 'Cuenoud Bernard', 'Guiraud Benjamin', 'Sarl Uni Distribution', 'Buytaert Jean', \"Frank'S International, Inc.\", 'Hining Ira Eugene', 'Heemstra Harrie', 'Neopost Technologies', 'Sytema Herman', 'Lasfargues Sarah', 'Legoff Emilie', 'Lafarg Sara', 'Mortimer Stiv', 'Zhermen Bertran', 'Kazalidzhi Kleman', 'Hexcel Composites Ltd', 'Kheksel Kompozits Limited', 'Germain Bertrand', 'Casaliggi Clement', 'Fianium Ltd', 'Fianium Limited', 'Yarrow Michael', 'Grudinin Anatoly', 'Clowes John', 'Dupriez Pascal', 'Groupe Guillin', 'Guillin Francois', 'Bernard David', 'Lelievre Etienne', 'Centre Leon Berard', 'Bartholin Laurent', 'Augert Arnaud', 'Besnainou Charles', 'Frelat Joël', 'Mamou-Mani Adrien', 'Alstom Transport Sa', 'Graff De Faget Sandrine', 'Assistance Publique Hopitaux De Paris', 'Institut National De La Recherche Agronomique', \"Institut National De Recherche En Sciences Et Technologies Pour L'Environnement Et L'Agriculture (Irstea)\", 'Peugeot Citroen Automobiles Sa', \"Commissariat A L'Energie Atomique Et Aux Energiesalternatives\", 'Sanofi Sa', 'El Hassani Chakib', 'Ibrizheniks Sa', 'Hybrigenics Sa', 'Dow Corning Toray Silicone Co Ltd', 'De Buyl Francois', 'Dow Corning Toray Co Limited', 'Amadeus S A S', 'Hu Hao', 'St-Ericsson Sa', 'Habimana Jean De La Croix', 'Wiegand James A.', 'St Microelectronics Res Dev', 'Centre National De La Recherche Scientifique - Cnrs -', 'Celsius X Vi Ii', 'Rondo Ag', 'Agco Sa', 'Agco Sa', 'Ehssilor Internas Onal Kompani Zheneral D Optik', 'Essilor International Compagnie Generale D Optique', 'Antia Therapeutics S A', 'Centre National De La Recherche Scientifique Cnrs', 'Universite Claude Bernard Lyon 1 Ucbl', 'Centre National De La Recherche Scientifique', 'Kheksel Kompozits S A S', 'Hexcel Composites S A S', 'Nkt Photonics A S', 'Institut National De La Sante Et De La Recherche Medicale (Inserm)', 'Centre National De La Recherche Scientifique (Cnrs)', 'Centre National De La Recherche Scientifique', 'Universite Pierre Et Marie Curie Paris 6']\n"
     ]
    }
   ],
   "source": [
    "L = open(r\".\\longNameRecordsOfauthorshipRecords_20000.json\",'r',encoding=\"ISO-8859-1\")\n",
    "C1 = []\n",
    "C2 = processList(L,C1)\n",
    "L.close()\n",
    "\n",
    "S = open(r\".\\shortNameRecordsOfauthorshipRecords_20000.json\",'r',encoding=\"ISO-8859-1\")\n",
    "C3 = processList(S,C2)\n",
    "print(C3)\n",
    "S.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
